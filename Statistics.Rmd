---
title: "Statistics"
author: "Vincent Courtois"
date: "20/02/2022"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## MIT Statistics and Data Science MicroMasters
## Statistics

Quadratic risk = Variance - $bias^2$
Delta Method  
Asymptotic variance of an estimator

Confidence Intervals:

* Conservative (worst variance possible)
* Solve (Root for $ax^2+bx+c$ are $\frac{-bÂ±\sqrt{b^2-4ac}}{2a}$
* Plugin (use point estimate in unknown variance)



 Total Variation Distance:
 
 * $TV(P_\theta,P_{\theta'}) = \frac {1}{2}\sum_{k \in E} |p_\theta(k)-p_{\theta'}(k)|$
 * $TV(P_\theta,P_{\theta'}) = \frac {1}{2}\int_E |f_\theta(x)-f_{\theta'}(x)|dx$
 
 Kullback-Leibler (KL) divergence:

* $KL(P_{\theta},P_{\theta'}) = \sum_{k \in E}p_\theta(k)log(\frac{p_\theta(k)}{p_\theta'(k)})$ if E is discrete
* $KL(P_{\theta},P_{\theta'}) = \int_Ef_\theta(x)log(\frac{f_\theta(x)}{f_\theta'(x)})dx$ if E is continuous

A function is concave if its second derivative is negative.
