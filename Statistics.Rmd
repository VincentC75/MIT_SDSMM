---
title: "Statistics"
author: "Vincent Courtois"
date: "20/02/2022"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## MIT Statistics and Data Science MicroMasters
## Statistics

$Quadratic risk = Variance - bias^2$  
Delta Method
Let $Z_n$ be a sequence of random variables such that $\sqrt{n}(Z_n-\theta) \lim_{n \to \infty} N(0,\sigma^2)$
g continuously differentiable at $ \theta$
$\sqrt{n}(g(Z_n)-g(\theta)) \lim_{n \to \infty} N(0,g'(\theta)^2\sigma^2)$


Asymptotic variance of an estimator

Confidence Intervals:

* Conservative (worst variance possible)
* Solve (Root for $ax^2+bx+c$ are $\frac{-bÂ±\sqrt{b^2-4ac}}{2a}$
* Plugin (use point estimate in unknown variance)

Hypothesis testing

Methods of estimation:
  - Maximum likelihood exstimator
  - Method of moments
  - M-estimators

Distance measure / probability distributions

 Total Variation Distance:
 
 * $TV(P_\theta,P_{\theta'}) = \frac {1}{2}\sum_{k \in E} |p_\theta(k)-p_{\theta'}(k)|$
 * $TV(P_\theta,P_{\theta'}) = \frac {1}{2}\int_E |f_\theta(x)-f_{\theta'}(x)|dx$
 
 Kullback-Leibler (KL) divergence:

* $KL(P_{\theta},P_{\theta'}) = \sum_{k \in E}p_\theta(k)log(\frac{p_\theta(k)}{p_\theta'(k)})$ if E is discrete
* $KL(P_{\theta},P_{\theta'}) = \int_Ef_\theta(x)log(\frac{f_\theta(x)}{f_\theta'(x)})dx$ if E is continuous

A function is concave if its second derivative is negative.

**MLE Estimator**
likelihood = product of likelihood of all observations.
maximize log likelihood:

* check function is concave (second derivative is negative)
* derive and set to 0 => solve for MLE estimator.

**EM algorithm** 

**Fisher Information**:

$l(\theta)$ = log likelihood for **one** observation
$I(\theta) = Var[l'(\theta)] = -E[l''(\theta)]$

* Parametric Hypthesis testing

Wald test (asymptotic): $W = \frac{\hat{\theta} - \theta_0}{\sqrt{\widehat{var(\hat{\theta})}}} \rightarrow N(0,1)$

Student's T test (non asymptotic): $T = \sqrt{n} \frac{\bar{X_n} - \mu_0}{S_n} \sim t_{n-1}$

Cochrans's theorem: for iid Gaussian random variables with mean $\mu$ and varaince $\sigma^2$, the sample mean and the sample variance are independent.

U standard Gaussian RV, V $\chi^2$ RV with d degrees of freedom. If U and V are independent, $\sqrt{d}\frac{U}{\sqrt{V}}$ is a Student t random variable with d degrees of freedom.

$\chi^2$ test => used to check wether a random variable has any distribution over a finite space E.

likelihood ratio => used to obtain test with *asymptotic* levels.




