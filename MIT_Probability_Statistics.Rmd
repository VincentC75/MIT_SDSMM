---
#title: "USCD Statistics notes"
#author: "Vincent Courtois"
geometry: "left=0.5cm,right=0.5cm,top=1cm,bottom=0.5cm"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**Sets**  
$A \cap (B \cup C) = (A \cap B) \cup (A \cap C), A \cup (B \cap C) = (A \cup B) \cup (A \cap C)$  
De Morgan's laws: $(A \cup B)^c = A^c \cap B^c$,
$(A \cap B)^c = A^c \cup B^c, (\cap_nS_n)^c=\cup_nS_n^c, (\cup_nS_n)^c=\cap_nS_n^c$  
Bonferroni's inequality: $P(A_1\cap A_2)\ge P(A_1) + P(A_2) -1, P(A_1\cap A_2 ...\cap A_n)\ge P(A_1) + P(A_2) + ... + P(A_n) -(n-1)$  
inclusion-exclusion:  
$P(A \cup B) = P(A) + P(B) - P(A \cap B)$  
$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) -P(B \cap C) + P(A \cap B \cap C)$  
$P(\cup_{i=1}^nAi) = \sum_i P(A_i) - \sum_{i<j}P(A_i \cap A_j) + \sum_{i<j<k}P(A_i \cap A_j \cap A_k)-...+(-1)^{n+1}P(A_1 \cap ... \cap A_n)$  
$A.(B\cup C)= A.B \cup A.C$  
$A.(B\cap C)= A.B \cap A.C$  
$A.(B-C)= A.B - A.C$  
difference: $A - B = A \cap B^c$  
symmetric difference: $A \Delta B = (A-B) \cup (B-A)$, elements exactly in one set, not both  
Cartesian product: AxB = {(a,b) | a in A, b in B }  
Cartesian power: AxAx...xA = $A^n$
Power set: collection of all subsets of S, 1-1 relation with $\{0,1\}^{|S|}$  
functions from A to B: $|B|^{|A|}$  

**Permutations and combinations**  
Permutations: $n!$, Partial permutations: $\frac{n!}{(n-k)!}$, Combinations: $\frac{n!}{k!(n-k)!}$  
All possible subsets of a set: $\sum_{i=0}^n \binom{n}{k} = 2^n$  
_Binomial coefficient_: $\binom{n}{k} = \frac{n!}{k!(n-k)!}= \frac{n}{k} \binom{n-1}{k-1}$  
_Multinomial coefficient_: $\binom{n}{k_1,k_2,k_3} = \frac{n!}{k_1!k_2!k_3!}$  
Binomial theorem: $(a+b)^n = \sum_{i=0}^n \binom{n}{i}a^{n-i}b^i$  
Multinomial theorem: $(a_1+a_2+...+a_m)^n = \sum_{k_1+k_2+...+k_m=n}\binom{n}{k_1,k_2,...,k_m}\prod_{t=1}^{m}a_t^{k_t}$  
Pascal's identity: $\binom{n+1}{k}=\binom{n}{k}+\binom{n}{k-1}$  
_Stirling's approximation_: $n! \sim \sqrt{2\pi n}(\frac{n}{e})^n$  
$m^n = (1+1+...+1)^n=\sum_{k_1+...+k_m=n}\binom{n}{k_1,k_2,...,k_m}$  
ways to write n as a sum of k positive integers, when order matters: $\binom{n-1}{k-1}$  
ways to write n as a sum of any number of positive integers: $2^{n-1}$  
ways to write n as a sum of k non negative integers, when order matters: $\binom{n+k-1}{k-1}$  

**Series and limits**  
infinite series $\sum_{i=0}^\infty a_i$ converges if $\sum_{i=0}^\infty |a_i| < \infty$. In that case the order of terms can be permuted.  
Geometric: $\sum_{0}^{+\infty} aq^n = \frac{a}{1-q}, q<1$. Partial sum $\sum_{0}^{n-1} aq^n = \frac{a(1-q^n)}{1-q}$ for q<1, $an$ for q=1.
Taylor expension, Maclaurin for exponential : $e^x = \sum_{n=0}^{\infty}\frac{x^n}{n!}$  
Euler number: $\lim_{n\to\infty} (1 + \frac{1}{n})^n = e, \lim_{n\to\infty} (1 + \frac{a}{n})^n = e^a$  
Harmonic Sum: $\frac{1}{1}+\frac{1}{2}+...+\frac{1}{n} \to ln(n)+0.5777$  
$\frac{1}{1^2}+\frac{1}{2^2}+...+\frac{1}{(n-1)^2}+\frac{1}{n^2} \to \frac{\pi^2}{6}$

**Integration by parts**  
$\int u.dv = uv - \int v.du$  
$\int_0^1\Theta^\alpha(1-\Theta)^\beta d\theta=\frac{\alpha!\beta!}{(\alpha+\beta+1)!}$

**Probability**  
_3 Axioms of probability_: Non negativity ($P(A)\ge0$), Additivity ($P(A\cup B) = P(A)+P(B)$ if A,B disjoint), Normalization ($P(\Omega)=1$)  
_Total probability_: $P(A)=P(B).P(A|B)+P(B^c).P(A|B^c)$ (extends to partition of $\Omega$)  
_Multiplication rule_: $P(A\cap B)=P(A).P(B|A) = P(B).P(A|B)$ (extends to more than 2)  
_Conditional probability_: $P(A|B) = \frac{P(A\cap B)}{P(B)}$  
_Baye's Rule_: $P(A|B) = \frac{P(A).P(B|A)}{P(B)}$    
_Marginal_: $p(x) = \sum_yp(x,y)$  
_Independence_: $P(A\cap B) = P(A).P(B)$. rows and columns proportional in joint probability matrix  
Independence of collection of events: $P(A_1\cap A_2 ...\cap A_m)=P(A_1).p(A_2)...P(A_m)$ for any subset of the collection.   
Independence does not imply conditional independence. Pairwise independence does not imply independence of collection.  

**Expectation**  
_Linearity of expectation_: $E(aX+b)=aE(X)+b$  
$Y=g(X), E(Y)=\sum_xg(x)p_x$  
$E(X+Y)=E(X)+E(Y), usually, E(XY) \neq E(X).E(Y) (unless X,Y independent)$  
If X,Y independent, $E[XY]=E[X].E[Y], E[g(x)h(y)]=E[g(x)].E[h(y)], Var(X+Y) = Var(X) + Var(Y)$  
_Total Expectation Theorem_: $E[X] = P(A_1).E[X|A_1]+...+P(A_n).E[X|A_n]$  
_Law of Iterated Expectations_: $E[E[X|Y]] = E[X]$  
Sum of a random number of random variables : E[S] = E[N]*E[X]

**Variance**  
$V(X)=E[(X-\mu)^2] = E[X^2] - (E[X])^2$  
$V(aX+b) = a^2V(X)$  
$V(aX+bY) = a^2V(X)+b^2V(Y)+2ab.cov(X,Y)$  
$V(X_1+...+X_n) = \sum_{i=1}^nV(X_i) + \sum_{i\neq j}Cov(X_i,X_j)$  
_Law of Total Variance_: $Var(X) = E[Var(X|Y)] + Var(E[X|Y])$  
Sum of a random number of random variables : $V(S) = E[N]*V(X) + (E[X])^2*V(N)$  

**Covariance and Pearson's correlation coefficient**  
$cov(X,Y) = E[(X-E[X]).(Y-E[Y])] = E[XY] - E[X].E[Y]$: amount X and Y vary together  
$\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X\sigma_Y}$  
$\rho_{aX+b,cY+d} = sign(ac).\rho_{X,Y}$  
Cauchy-Schwartz inequality: $|E(XY)| \leq \sqrt{E(X^2)}.\sqrt{E(Y^2)}, |\sigma_{x,Y}| \leq \sigma_X.\sigma_Y, |\rho_{X,Y}| \leq 1$  
Independent implies uncorrelated. Uncorrelated does not imply independent.  

...|Discrete|Continuous|
---|---|---|
Mass vs Density|PMF: $p_X(x), p_{X,Y}(x,y), p_{X|Y}(x|y), p_{X|A}(x|A)$|PDF: $f_X(x), f_{X,Y}(x,y), f_{X|Y}(x|y), f_{X|A}(x|A)$|
Expectation|$E[X], E[X|A], E[X|Y=y]$|$E[X], E[X|A], E[X|Y=y]$|
Expected value rule|$E[g(X,Y), E[g(X,Y)|A],E[G(x,y)|Z=z]$|$E[g(X,Y), E[g(X,Y)|A],E[G(x,y)|Z=z]$|
Linearity|$E[aX+bY]=aE[X]+bE[Y]$|$E[aX+bY]=aE[X]+bE[Y]$|
Independence|$p_{X,Y}=p_X \cdot p_Y$|$f_{X,Y}=f_X\cdot f_Y$|
.|$E[XY]=E[X]\cdot E[Y]$|$E[XY]=E[X]\cdot E[Y]$|
.|$Var(X+Y)=Var(X)+Var(Y)$|$Var(X+Y)=Var(X)+Var(Y)$|
Multiplication Rule|$p_{X,Y,Z}=p_Z(z)\cdot p_{Y|Z}(y|z)\cdot p_{X|Y,Z}(x|y,z)$|$f_{X,Y,Z}=f_Z(z)\cdot f_{Y|Z}(y|z)\cdot f_{X|Y,Z}(x|y,z)$|
Total Probability Theorem|$p_X(x)=\sum_yp_Y p_{X|Y}(x|y)$|$f_X(x)=\int_yf_Y f_{X|Y}(x|y)dy$|
Total Expectation Theorem|$E[X]=\sum_y p_Y(y)\cdot E[X|Y=y]$|$E[X]=\int_y f_Y(y)\cdot E[X|Y=y]dy$|
Distributions|bernouilli, indicators, uniform, binomial, geometric|uniform, geometric, exponential, normal, beta|


Discrete|support|pmf|cdf|mgf|mean|variance|
:---|:-----|:--------|:---|:------|:---|:---|
Bernouilli|$k\in \{0,1\}$|$p_p(k)=p^k (1-p)^{1-k}$||$1+p(e^t-1)$|$p$|$p(1-p)$
Binomial|$k\in \{0,1,...,n\}$|$p_{n,p}(k)=\binom{n}{k}p^k(1-p)^{n-k}$||$[1+p(e^t-1)]^n$|$np$|$np(1-p)$
HyperGeometric|$k\in \{1,2,,...,n\}$|$p_p(k)=\frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}$|||$\frac{nK}{N}$|$n\frac{K}{N}\frac{N-K}{N}\frac{N-n}{N-1}$
Geometric|$k\in \{1,2,,...,n\}$|$p_p(k)=p(1-p)^{k-1}$|$1-(1-p)^k$||$\frac{1}{p}$|$\frac{1-p}{p^2}$
Poisson|$k \geq 0, \lambda \geq 0$|$p_{\lambda}(k)=\frac{\lambda^ke^{-\lambda}}{k!}$||$e^{\lambda(e^t-1)}$|$\lambda$|$\lambda$
Negative Binomial|$k\in \{0,1,...,n\}$|$p_{r,p}(k)=\binom{k+r-1}{k}(1-p)^kp^r$|$I_p(r,k+1)$||$\frac{r(1-p)}{p}$|$\frac{r(1-p)}{p^2}$|

Continuous|support|pdf|cdf|mgf|mean|variance|
:---|:-----|:--------|:---|:------|:---|:---|
Uniform|[a,b]|$\frac{1}{b-a}$|$\frac{x-a}{b-a}$||$\frac{a+b}{2}$|$\frac{(b-a)^2}{12}$
Exponential|$x\in[0,\infty)$|$f_{\lambda}(x)=\lambda e^{-\lambda x}$|$1-e^{-\lambda x}$||$\frac{1}{\lambda}$|$\frac{1}{\lambda^2}$
Gaussian|$x\in \mathbb{R}$|$f_{(\mu,\sigma^2)}(x) = \frac{1}{\sigma \sqrt{2 \pi} } e^{-\frac{(x - \mu)^2}{2 \sigma^2} }$|$\Phi(x)$|$e^{\mu t+\frac{\sigma^2t^2}{2}}$|$\mu$|$\sigma^2$
Beta|$x\in[0,1]$|$f_{\alpha,\beta}(x) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$|$I_x(\alpha,\beta)$||$\frac{\alpha}{\alpha+\beta}$|$\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$|
Gamma|$x\in(0,\infty)$|$f_{\alpha,\beta}(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$|||$\frac{\alpha}{\beta}$|$\frac{\alpha}{\beta^2}$
Erlang|$x\in(0,\infty)$|$f(x;k,\lambda)=\frac{\lambda^k x^{k-1} e^{-\lambda x}}{(k-1)!}$|$1-\sum_{n=0}^{k-1}\frac{1}{n!}e^{-\lambda x}(\lambda x)^n$||$\frac{k}{\lambda}$|$\frac{k}{\lambda^2}$

geometric and exponential distributions are memoryless. exponential extends geometric to continuous values.  
Poisson binomial, extends binomial, $p_i$ different but known for each trial. $E(X)=\sum_ip_i, V(X)=\sum_ip_i(1-p_i)$  
Poisson approximates Binomial for $\lambda=np, n\gg1 \gg p$  
Binomial: n Bernoulli trials, count successes. Binomial ($n, p=\lambda /n$ ) converges to Poisson($\lambda$). Hypergeometric ~ binomial without replacement.   
Geometric: Number of Bernoulli trials until first success. Any discrete memoryless distribution is geometric.  
Negative binomial distribution: Number of Bernouilli trials until r's success $\binom{n-1}{r-1}p^r(1-p)^{n-r}$  
Gamma distribution: $\alpha$ is the shape parameter, $\beta$ is the inverse scale parameter.  
A chi squared distribution with n degrees of freedom is a special case of the gamma distribution $\Gamma(n/2,1/2)$  
Linear transformations of normal distributions are normal.  
$X \sim N(\mu,\sigma^2), Y=aX+b, Y \sim N(a\mu+b,a^2\sigma^2), Z=\frac{X-\mu}{\sigma} \sim N(0,1), 68, 95, 99.7 rule$  
Normal approximation of binomial: $X \sim B_{n,p} \sim N(np,np(1-p))$. 1/2 correction for integer approximation.  

**Derived Distributions, Functions of Random variables**  
$Y=aX+B: p_Y(y)=p_X(\frac{y-b}{a}), f_Y(y) = \frac{1}{|a|}f_X(\frac{y-b}{a})$  
For Y=g(X), first find the CDF and then derive it to get the pdf.  
if g is monotonic, $Y=g(X), h(y)=g^-1(y), f_Y(y)=f_X(h(y)).|h'(y)| = \frac{1}{|g'(x)|}f_X(x)|_{y=g(x)}$  
_Convolution formula_: $Z=X+Y, p_Z(z)=\sum_xp_X(x)\cdot p_Y(z-x), f_Z(z)=\int_xf_X(x)\cdot f_Y(z-x)dx$   
Sum of a Random number of iid variables X. $S=\sum_NX_i, E[S]=E[N]E[X], Var(S) = E[N]Var(X) + E[X]^2Var(N)$  
Absolute value of Y=|X|: $p_Y(y) = p_X(y) + p_X(-y), f_Y(y) = f_X(y) + f_X(-y)$  

**Bayesian Inference**  
Problem data: $p_\theta(.), p_{X|\Theta(.|.)}$  
Given value x of X, find $p_{\Theta|X}(.|x)$ using one of 4 variations of Bayes rule, pmf or pdf.  
Maximum a posteriori (MAP) estimator: $\hat{\Theta}_{MAP}$ maximizes $p_{\Theta|X}(\theta|x)$  
Least Mean Squares (LMS) estimator: $\hat{\Theta}_{LMS} = E[\Theta|X=x]$  
Performance estimation of estimator: $P(\hat{\Theta}\neq \Theta), E[(\hat{\Theta}-\Theta)^2],P(\hat{\Theta}\neq \Theta|X=x), E[(\hat{\Theta}-\Theta)^2|X=x]$

**Moment Generating Functions**  
$M_X(t)=E[e^{tX}], M_{aX+b} = e^{bt}M_X(at), E[X]=M_X'(0), E[X^n]=M_X^{(n)}(0)$  
Properties: Positive, M(0)=1, Finite support, MGF of sum is product of MGF, if same MGF then same distribution.

**Inequalities**  
Markov's: $X\geq0, \forall \alpha \geq 1, P(X \geq \alpha \mu) \leq \frac{1}{\alpha}, \forall a \geq \mu, P(X \geq a) \leq \frac{\mu}{a}$. Decreases linearly  
Chebyshev's: $\forall X \forall \alpha \geq1, P(|X-\mu|\geq\alpha\sigma) \leq \frac{1}{\sigma^2}, \forall a \geq \sigma, P(|X-\mu|) \geq a) \leq \frac{\sigma^2}{a^2}$. Decreases quadratically  
Chernoff Bound: $X\sim B_{p,n}, P(X \geq (1+\delta)\mu) \leq e^{-\frac{\delta^2}{2+\delta}\mu}, P(X \leq (1-\delta)\mu) \leq e^{-\frac{\delta^2}{2}\mu}$. Decreases exponentially in n.

**Weak law of large numbers**  
$X^n=X_1,...X_n$ iid samples with finite $\mu$ and $\sigma^2$, $P(|\overline{X^n}-\mu|\geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}\frac{1}{n}$: $\overline{X^n}$ converges in probability to $\mu$.  
Generalizes with $\mu=\frac{1}{n}\sum_i\mu_i, \sigma^2=\frac{1}{n}\sum_i\sigma_i^2$ when means and sd differ.  

**Central Limit Theorem**
Normalized sum of random variables is roughly normal (discrete, continuous, mixed, under mild conditions).  
X_1,X_2... be iid with finite $\mu$ and $\sigma$, as $n \to \infty$ the distribution of $\frac{X_1+X_2+...+X_n-n\mu}{\sigma\sqrt{n}}$ approaches $N(0,1)$  
$P(\overline{X^n} \leq \alpha) \approx \Phi(\frac{\alpha - \mu}{\sigma / \sqrt{n}})$
Central Limit Theorem: $\bar{X}\sim\mathcal{N}(\mu,\frac{\sigma^2}{n})$

**Bernouilli and Poisson process**  

Process|Times of Arrival|Arrival Rate|PMF of # of Arrivals|Interarrival Time Distribution|Time to kth Arrival|
---|---|---|---|---|---|
Poisson|Continuous|$\lambda$/unit time|Poisson|Exponential|Erlang|
Bernouilli|Discrete|p/trial|Binomial|Geometric|Pascal (negative binomial)|
_Poisson Process_: $\lambda$ rate per time unit, $\tau$ time interval. $E[N_\tau]=\lambda \tau, Var(N_\tau)=\lambda\tau, P(k,\tau)=\frac{(\lambda \tau)^k e^{-\lambda \tau}}{k!}, f_{Y_k}=\frac{\lambda^k y^{k-1}e^{-\lambda y}}{(k-1)!}$ (Erlang).   
The sum of independent Poisson random variables with meams/parameters $\mu$ and $\nu$ is Poisson with mean/parameter $\mu+\nu$.  
In a merged Poisson process with mean/parameter $\mu+\nu$, the probability it originates from the first or second4 process is $\frac{\mu}{\mu+\nu}$ and $\frac{\nu}{\mu+\nu}$.  
_Random incidence_ paradox in the poisson process. E[inter arrival time] = $\frac{1}{\lambda}$ but if measured at random time = $\frac{2}{\lambda}$ because of bias tower longer intervals.  

**Markov chains**  
_Markov property_: The future only depend on the current state, not hte past.  
$r_{ij}(n) = \sum_{k=1}^m r_{ik}(n-1)p_{kj}$  
state i is _recurrent_ if starting from i and from wherever you go the is a path back to i. Otherwise state is _transient_.  
_Recurrent class_: collection of recurrent states communicating only between each other.  
The states in a recurrent class are _periodic_ if they can be grouped into d>1 groups so that all transitions from one group lead to the next group. Cannot be periodic if there are self transitions.  
Convergence to steady state probabilities if all recurrent states are in a single class and this class is not periodic.  
_Balance equations_: $\pi_j = \sum_{k=1}^m \pi_k p_{kj}$.  

**Statistics - Distance between probalility distributions**  
_Total Variation_ $TV(P_\theta,P_{\theta'})=max_{A \in E}|P_\theta(A)-P_{\theta'}(A)|$  
_Kullback-Leibler (KL) divergence_ $KL(P_\theta,P_{\theta'})=\sum_{x \in E}p_\theta(x)log(\frac{p_\theta(x)}{p_{\theta'}(x)})$ if discrete, $=\int_Ef_\theta(x)log(\frac{f_\theta(x)}{f_{\theta'}(x)})dx$ if continuous.  
 
**Statistics - Estimation**  
A _statistic_ is any measurable function of the sample  
An _estimator_ of $\theta$ is a statistic $\hat{\theta} = \hat{\theta}(X_1,X_2,...X_n)$ whose expression does not depend on $\theta$.  
An estimator $\hat{\theta}$ is consistent if it converges to $\theta$. (weakly if convergence in probability, strongly if almost surely).  
_Bias_ of an estimator: $Bias(\hat{\theta}) = E[\hat{\theta}]-\theta$  
_Jensens's inequality_: if f is convex, $E[f(x)] \ge f(E[x])$. If g is concave, $E[g(x)] \le g(E[x])$.  
_Quadratic risk_$= Bias^2 + Variance = E[(\hat{\theta_n}-\theta)^2]$ = MSE, Mean Squared Error. Bias and variance trade off.   
Sample Mean: $E[\overline{X}] = \mu$ (unbiased), $V(\overline{X})=\frac{\sigma^2}{n}$, $MSE_\mu(\overline{X})=Bias_\mu^2(\overline{X})+V(\overline{X})=\frac{\sigma^2}{n}$  
Raw $S^2=\frac{1}{n}\sum(x_i-\overline{x})^2=\frac{1}{n}\sum x_i^2 - \overline{x}^2$  
Bessel's correction: $S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2=\frac{1}{n-1}(\sum_{i=1}^nX_i^2-n\overline{X}^2)$  
No general unbiased estimator for standard deviation.  

**Statistics - Confidence intervals**  
Confidence intervals: $P(-a\leq Z \leq a) = 2\Phi(a) - 1$ (python=norm.cdf)  
Given desired probability, $\Phi(z_p)=\frac{1+p}{2}, z_p =\Phi^{-1}(\frac{1+p}{2})$ (python=norm.ppf)  
Confidence interval for the mean: $(\overline{X}-z_p\frac{\sigma}{ \sqrt{n}}, \overline{X}+z_p\frac{\sigma}{\sqrt{n}})$  
If $\sigma$ is unknown, replace by S. $\frac{\overline{X}-\mu}{S/\sqrt{n}} \sim$ Student's t-distribution whith n-1 degrees of freedom. (python=t.pdf(x,n-1))  
Confidence interval for the mean if $\sigma$ unknown: $(\overline{X}-t_{p,n-1}\frac{S}{ \sqrt{n}}, \overline{X}+t_{p,n-1}\frac{S}{\sqrt{n}}), t_{p,n-1}=F_{n-1}^{-1}(\frac{1+p}{2})$  
_Delta Method_: $\sqrt{n}(g(\bar{X_n})-g(\theta)) \to N(0,(g'(\theta))^2 \sigma^2) = N(0,(g'(E[X_i]))^2 Var(X_i))$  

**Statistics - Hypothesis testing**  
Hypothesis = Assumption about parameters. simple or composite, one-sided or two-sided.  
Null hypothesis, $H_0$ is the _status_.  
Determine the distribution of a test statistic under $H_0$. Reject $H_0$ if small probability.
Type-I error: reject $H_0$ when $H_0$ is true.  
Significance level $\alpha$ (probability of Type I error): If $H_0$ is true, accept $H_A$ with probability $\leq \alpha. P_{H_0}(X\geq X_\alpha) \leq \alpha$.  
Operating Characteristic $\beta$ (probability of Type II error). $1-\alpha$ = confidence level, $1-\beta$ = power.  
Method: critical-value or p-value.  
z-test ($\sigma$ known) or t-test ($\sigma$ unknown).  

**Matrices and vectors**  
u, v (1,d) column vectors. $u^T.v$ is a scalar, inner product or dor product. $u.v^T$ is (d,d) matrix = outer product.  
determinant of matrix = ad-bc = product of eigenvalues.  
trace of matrix = sum of diagonal entries = sum of eigenvalues.  

**Convergence**  
_Convergence in probability_: a sequence $Y_n$ converges in probability to a number a if for any $\epsilon >0, \lim P(|Y_n-a| \ge \epsilon) = 0$.  
Convergence in probability of random variables does not imply converge of expectations.
Convergence in distribution: the sequence CDF converges to a CDF
Convergence with probability 1 (almost surely ?) 
